---
title: çˆ¬è™«æ¡†æ¶â€”â€”Scrapyå¿«é€Ÿä¸Šæ‰‹
date: 2020/6/14
description: çˆ¬è™«æ¡†æ¶â€”â€”Scrapyå¿«é€Ÿä¸Šæ‰‹
top_img: 'https://fabian.oss-cn-hangzhou.aliyuncs.com/img/QKkrGgWpRlzFATx.jpg'
cover: 'https://fabian.oss-cn-hangzhou.aliyuncs.com/img/QKkrGgWpRlzFATx.jpg'
categories:
  - python
tags:
  - çˆ¬è™«
  - Scrapy
abbrlink: 1097
---

# çˆ¬è™«æ¡†æ¶â€”â€”Scrapyå¿«é€Ÿä¸Šæ‰‹

## ä¸€ã€Scrapy ç®€ä»‹

>Scrapy æ¡†æ¶
>		Scrapyæ˜¯ç”¨çº¯Pythonå®ç°ä¸€ä¸ªä¸ºäº†çˆ¬å–ç½‘ç«™æ•°æ®ã€æå–ç»“æ„æ€§æ•°æ®è€Œç¼–å†™çš„åº”ç”¨æ¡†æ¶ï¼Œç”¨é€”éå¸¸å¹¿æ³›ã€‚æ¡†æ¶çš„åŠ›é‡ï¼Œç”¨æˆ·åªéœ€è¦å®šåˆ¶å¼€å‘å‡ ä¸ªæ¨¡å—å°±å¯ä»¥è½»æ¾çš„å®ç°ä¸€ä¸ªçˆ¬è™«ï¼Œç”¨æ¥æŠ“å–ç½‘é¡µå†…å®¹ä»¥åŠå„ç§å›¾ç‰‡ï¼Œéå¸¸ä¹‹æ–¹ä¾¿ã€‚Scrapy ä½¿ç”¨äº† Twisted å¼‚æ­¥ç½‘ç»œæ¡†æ¶æ¥å¤„ç†ç½‘ç»œé€šè®¯ï¼Œå¯ä»¥åŠ å¿«æˆ‘ä»¬çš„ä¸‹è½½é€Ÿåº¦ï¼Œä¸ç”¨è‡ªå·±å»å®ç°å¼‚æ­¥æ¡†æ¶ï¼Œå¹¶ä¸”åŒ…å«äº†å„ç§ä¸­é—´ä»¶æ¥å£ï¼Œå¯ä»¥çµæ´»çš„å®Œæˆå„ç§éœ€æ±‚ã€‚

- è™½ç„¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç‹¬ç«‹çš„ python è„šæœ¬æ¥ç¼–å†™æˆ‘ä»¬çš„çˆ¬è™«ï¼Œä½†æ¡†æ¶çš„ä½¿ç”¨æ— ç–‘ä¼šå¤§å¤§**ç®€åŒ–æˆ‘ä»¬çš„çˆ¬è™«å¼€å‘**ä»¥åŠ**åŠ å¿«æˆ‘ä»¬çš„çˆ¬å–é€Ÿåº¦**ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨åœ¨æ¡†æ¶ä¸­è‡ªå®šä¹‰ç»„ä»¶æ¥æ§åˆ¶å’Œä¼ªè£…æˆ‘ä»¬çš„çˆ¬è™«ï¼Œä»¥è¾¾åˆ°ååçˆ¬çš„ç›®çš„ã€‚

æˆ‘ä»¬è¿™é‡Œä»¥ä¸€ä¸ªå›¾ç‰‡ç½‘ç«™ä¸ºä¾‹è¿›è¡Œçˆ¬å– -------> https://www.mzitu.com/ (ç”±äºä¸€äº›åŸå› æˆ‘å°±ä¸æŠŠçˆ¬å–çš„ç»“æœå›¾è´´ä¸Šå»äº†ğŸ¤)

## äºŒã€å®‰è£…ä¾èµ–åˆ›å»ºé¡¹ç›®

~~~shell
# ä¸‹è½½å®‰è£…ä¾èµ–(å¦‚æœé€Ÿåº¦å¤ªæ…¢å¯ä»¥é…ç½®é•œåƒ)
pip install Scrapy
# åˆ›å»ºçˆ¬è™«é¡¹ç›®
scrapy startproject MeiziTu(ä½ çš„çˆ¬è™«é¡¹ç›®åç§°)
# è¿›å…¥æ–‡ä»¶ç›®å½•
cd xxxxxx
# æ–°å»ºçˆ¬è™«æ–‡ä»¶
scrapy genspider meizitu(ä½ çš„çˆ¬è™«åç§°) mzitu.com(å¾…çˆ¬å–çš„åŸŸå)
# å¯åŠ¨çˆ¬è™«
scrapy crawl meizitu(ä½ çš„çˆ¬è™«åç§°)
~~~

è¿™æ¬¡å› ä¸ºæ˜¯ä¸€ä¸ªçˆ¬è™«å·¥ç¨‹ï¼Œæˆ‘ä»¬ç”¨ **Pycharm** æ‰“å¼€ï¼Œç„¶åä½ å°±å¯ä»¥çœ‹è§ä»¥ä¸‹ç›®å½•æ–‡ä»¶

![image-20200614195326900](https://fabian.oss-cn-hangzhou.aliyuncs.com/img/3msfunzHy7jCT6k.png)

- `spiders`ï¼šæ”¾ç½®ä½ çš„çˆ¬è™«è„šæœ¬çš„æ–‡ä»¶å¤¹
  - `_init_.py`ï¼šåŒ…çš„å£°æ˜æ–‡ä»¶
  - `meizitu.py`ï¼šä¸»è¦çš„çˆ¬è™«è„šæœ¬æ–‡ä»¶ï¼Œä½ çš„å¤§éƒ¨åˆ†ä¸šåŠ¡ä»£ç åœ¨è¿™é‡Œç¼–å†™
- `_init_.py`ï¼šåŒ…çš„å£°æ˜æ–‡ä»¶
- `items.py`ï¼šèµ„æºå°è£…æ–‡ä»¶
- `middlewares.py`ï¼šçˆ¬è™«ä¸­é—´ä»¶æ–‡ä»¶
- `pipelines.py`ï¼šç®¡é“å¤„ç†æ–‡ä»¶
- `settings.py`ï¼šé…ç½®æ–‡ä»¶
- `scrapy.cfg`ï¼šçˆ¬è™«çš„ä¸»é…ç½®æ–‡ä»¶

## ä¸‰ã€Scrapy æ¡†æ¶æ¶æ„

### Scrapy æ¶æ„å›¾

![image-20200614200344091](https://fabian.oss-cn-hangzhou.aliyuncs.com/img/6cl7CsITXOnb1Ao.png)

### Scrapy æµç¨‹å›¾

![image-20200614200428620](https://fabian.oss-cn-hangzhou.aliyuncs.com/img/6EhAkxmZ8BUiqVM.png)

### Scrapyæ¡†æ¶æ¨¡å—åŠŸèƒ½

- `Scrapy Engine`ï¼ˆå¼•æ“ï¼‰ï¼šScrapyæ¡†æ¶çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚è´Ÿè´£åœ¨Spiderå’ŒItemPipelineã€Downloaderã€Schedulerä¸­é—´é€šä¿¡ã€ä¼ é€’æ•°æ®ç­‰ã€‚
- `Spider`ï¼ˆçˆ¬è™«ï¼‰ï¼šå‘é€éœ€è¦çˆ¬å–çš„é“¾æ¥ç»™å¼•æ“ï¼Œæœ€åå¼•æ“æŠŠå…¶ä»–æ¨¡å—è¯·æ±‚å›æ¥çš„æ•°æ®å†å‘é€ç»™çˆ¬è™«ï¼Œçˆ¬è™«å°±å»è§£ææƒ³è¦çš„æ•°æ®ã€‚è¿™ä¸ªéƒ¨åˆ†æ˜¯æˆ‘ä»¬å¼€å‘è€…è‡ªå·±å†™çš„ï¼Œå› ä¸ºè¦çˆ¬å–å“ªäº›é“¾æ¥ï¼Œé¡µé¢ä¸­çš„å“ªäº›æ•°æ®æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼Œéƒ½æ˜¯ç”±ç¨‹åºå‘˜è‡ªå·±å†³å®šã€‚
- `Scheduler`ï¼ˆè°ƒåº¦å™¨ï¼‰ï¼šè´Ÿè´£æ¥æ”¶å¼•æ“å‘é€è¿‡æ¥çš„è¯·æ±‚ï¼Œå¹¶æŒ‰ç…§ä¸€å®šçš„æ–¹å¼è¿›è¡Œæ’åˆ—å’Œæ•´ç†ï¼Œè´Ÿè´£è°ƒåº¦è¯·æ±‚çš„é¡ºåºç­‰ã€‚
- `Downloader`ï¼ˆä¸‹è½½å™¨ï¼‰ï¼šè´Ÿè´£æ¥æ”¶å¼•æ“ä¼ è¿‡æ¥çš„ä¸‹è½½è¯·æ±‚ï¼Œç„¶åå»ç½‘ç»œä¸Šä¸‹è½½å¯¹åº”çš„æ•°æ®å†äº¤è¿˜ç»™å¼•æ“ã€‚
- `Item Pipeline`ï¼ˆç®¡é“ï¼‰ï¼šè´Ÿè´£å°†Spiderï¼ˆçˆ¬è™«ï¼‰ä¼ é€’è¿‡æ¥çš„æ•°æ®è¿›è¡Œä¿å­˜ã€‚å…·ä½“ä¿å­˜åœ¨å“ªé‡Œï¼Œåº”è¯¥çœ‹å¼€å‘è€…è‡ªå·±çš„éœ€æ±‚ã€‚
- `Downloader Middlewares`ï¼ˆä¸‹è½½ä¸­é—´ä»¶ï¼‰ï¼šå¯ä»¥æ‰©å±•ä¸‹è½½å™¨å’Œå¼•æ“ä¹‹é—´é€šä¿¡åŠŸèƒ½çš„ä¸­é—´ä»¶ã€‚
- `Spider Middlewares`ï¼ˆSpiderä¸­é—´ä»¶ï¼‰ï¼šå¯ä»¥æ‰©å±•å¼•æ“å’Œçˆ¬è™«ä¹‹é—´é€šä¿¡åŠŸèƒ½çš„ä¸­é—´ä»¶ã€‚

## å››ã€çˆ¬è™«æ–‡ä»¶

### 1. é»˜è®¤ç”Ÿæˆçš„çˆ¬è™«æ–‡ä»¶

```python
import scrapy

class MeizituSpider(scrapy.Spider):
    # ç»§æ‰¿äºSpiderç±»
    name = 'meizitu'
    # çˆ¬è™«åå­—
    allowed_domains = ['mzitu.com']
    # çˆ¬è™«å…è®¸çˆ¬å–çš„è®¿é—®
    start_urls = ['http://mzitu.com/']
    # çˆ¬è™«çš„å¼€å§‹è·¯å¾„

    def parse(self, response):
        # çˆ¬å–çš„å›è°ƒå‡½æ•°
        pass
```

### 2. ä¸¤ä¸ªå¸¸ç”¨çš„çˆ¬è™«ç±»

- `scrapy.Spider`ï¼šscrapy.Spideræ˜¯æ‰€æœ‰çˆ¬è™«ç±»çš„çˆ¶ç±»ã€‚
- `scrapy.spiders.CrawlSpider`ï¼šè§„åˆ™çˆ¬è™«æ˜¯çœç•¥äº†ä¸€èˆ¬çš„è§£æå·¥ä½œï¼Œå®ƒå®Œæˆäº†æ„Ÿå…´è¶£çš„è¿æ¥`<a href="">æ–‡æœ¬</a>`æå–ã€‚æŒ‰rulesä¸­ç»™å®šçš„Ruleè§„åˆ™è¿›è¡Œæå–è¿æ¥æ ‡ç­¾ä¸­çš„ä¿¡æ¯ï¼ˆhref, textï¼‰ã€‚

### 3. å¼€å§‹è§£æç»“æœ

~~~python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from MeiziTu.items import MeizituItem


class MeizituSpider(CrawlSpider):
    # è¿™é‡Œç”±äºé¡µé¢ç‰¹ç‚¹æˆ‘ä»¬å°†çˆ¬è™«ç»§æ‰¿è§„åˆ™çˆ¬è™«ç±»
    name = 'meizitu'
    allowed_domains = ['mzitu.com']
    start_urls = ['https://www.mzitu.com/']
    rules = (
        # å®šä¹‰çˆ¬è™«è§„åˆ™
        Rule(
            LinkExtractor(allow=r'https://www.mzitu.com/page/\d+/'),
            # é“¾æ¥é€‰æ‹©å™¨ é‡‡ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡ŒåŒ¹é…
            follow=True
            # è·Ÿè¿›é“¾æ¥
        ),
        Rule(
            LinkExtractor(allow=r'https://www.mzitu.com/\d+'),
            callback='parse_detail',
            # æŒ‡æ˜å›è°ƒå‡½æ•°æ¥å¤„ç†çˆ¬å–ç»“æœ
            follow=False
            # ä¸ç»§ç»­è·Ÿè¿›é“¾æ¥
        )
    )

    def parse_detail(self, response):
        # å¤„ç†ç»“æœçš„å›è°ƒå‡½æ•°
        title = response.xpath("//h2[@class='main-title']/text()").get()
        # xpathè¯­æ³•æå–ç»“æœ
        urls = []
        # å‡†å¤‡ç©ºåˆ—è¡¨å°è£…çˆ¬å–åˆ°çš„å›¾ç‰‡é“¾æ¥
        amount = response.xpath("//div[@class='pagenavi']//span/text()").getall()[-2]
        # æ‰¾åˆ°æ¯ä¸€å¥—çš„æ•°é‡
        base_url = response.xpath("//div[@class='main-image']/p/a/img/@src").get()
        # æ‰¾åˆ°æ¯ä¸€å¥—å›¾çš„urlè§„åˆ™
        for i in range(1, int(amount) + 1):
            # å¯¹urlè¿›è¡ŒæŒ‰è§„å¾‹çš„æ‹¼æ¥
            if i < 10:
                url = base_url.replace("1.jpg", str(i) + ".jpg")
            else:
                url = base_url.replace("01.jpg", str(i) + ".jpg")
            urls.append(url)
            # åŠ å…¥é›†åˆ
        item = MeizituItem(title=title, urls=urls)
        # å°è£…åˆ° item
        yield item
        # ä¼ åˆ°ä¸‹è½½ç»„ä»¶

~~~

>ç»“æœçš„è§£æè¿™é‡Œå¯ä»¥ä½¿ç”¨ **æ­£åˆ™è¡¨è¾¾å¼** ã€ **xpathè¡¨è¾¾å¼** å’Œ **cssé€‰æ‹©å™¨** è¿™äº›çš„å…·ä½“è¯­æ³•è¿™é‡Œä¸è¿‡å¤šèµ˜è¿°
>
>ç„¶åæ ¹æ®æµè§ˆå™¨çš„è°ƒè¯•å·¥å…·æ¥æŠ“å–è‡ªå·±æ‰€éœ€è¦çš„ä¿¡æ¯

### 4. çˆ¬è™«é…ç½®

~~~python
# å…³é—­å¯¹çˆ¬è™«åè®®çš„éµå®ˆ
ROBOTSTXT_OBEY = False

# å¼€å¯å»¶æ—¶ï¼Œé™ä½çˆ¬å–é¢‘ç‡ï¼Œé¿å…ç»™å¯¹æ–¹æœåŠ¡å™¨é€ æˆå¤ªå¤§å‹åŠ›ï¼Œå¹¶ä¸”é˜²æ­¢é¢‘ç‡å¤ªé«˜è¢«è¯†åˆ«ä¸ºçˆ¬è™«
DOWNLOAD_DELAY = 1

# é…ç½®éšæœºè¯·æ±‚å¤´è¿›è¡Œä¼ªè£… è¶Šå¤šè¶Šå¥½
USER_AGENT_LIST = [
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    ...
]
USER_AGENT = random.choice(USER_AGENT_LIST)
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en',
    'User-Agent': USER_AGENT
}

# æœ‰å¿…è¦çš„è¯ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œé…ç½®ä»£ç†ip
~~~

### 5. çˆ¬è™«è°ƒè¯•

- çˆ¬è™«å¯åŠ¨ï¼šå› ä¸ºscrapyæ˜¯ä»å‘½ä»¤è¡Œå¯åŠ¨ï¼Œè¿™è¾¹ä¸ºäº†æ–¹ä¾¿è°ƒè¯•ï¼Œæˆ‘ä»¬é€šè¿‡ python è„šæœ¬å¯åŠ¨

  ~~~python
  # ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªstart.pyæ–‡ä»¶
  from scrapy import cmdline
  
  # å¯åŠ¨å‘½ä»¤è¡Œå¹¶è¾“å…¥å‘½ä»¤
  cmdline.execute('scrapy crawl meizitu'.split())
  
  # è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è¿è¡Œè¿™ä¸ªè„šæœ¬æ¥å¯åŠ¨çˆ¬è™«
  ~~~

- çˆ¬è™«æµ‹è¯•ï¼šç”±äºçˆ¬è™«çš„æ¡†æ¶åœ¨è°ƒè¯•æ—¶é¢‘ç¹å¯åŠ¨å¾ˆéº»çƒ¦ï¼Œæˆ‘ä»¬è¿™é‡Œå¯ä»¥ä½¿ç”¨ scrapy æä¾›çš„å‘½ä»¤è¡Œå·¥å…·è¿›è¡Œæµ‹è¯•

  ~~~shell
  scrapy shell http://xxxxxxx.com(ä½ çš„ç›®æ ‡url)
  # æ¥ä¸‹æ¥å°±å¯ä»¥å¼€å§‹æµ‹è¯•ä½ çš„æ­£åˆ™è¡¨è¾¾å¼æˆ–è€…xpathè¯­æ³•
  ~~~

## äº”ã€èµ„æºå°è£…ä¸‹è½½

### 1. å°è£…

- æˆ‘ä»¬åœ¨`item.py`ä¸­å®šä¹‰æˆ‘ä»¬éœ€è¦ä¼ é€’çš„èµ„æº

  ~~~python
  import scrapy
  
  class MeizituItem(scrapy.Item):
      title = scrapy.Field()
      urls = scrapy.Field()
  ~~~

- ç„¶åå°±å¯ä»¥åœ¨çˆ¬è™«æ–‡ä»¶ä¸­å°†å…¶ä¼ é€’è¿‡æ¥

  ~~~python
  from MeiziTu.items import MeizituItem
  
  def prase(self, response):
      ...    
      item = MeizituItem(title=title, urls=urls)
      # å°è£…åˆ° item
      yield item
      # ä¼ åˆ°ä¸‹è½½ç»„ä»¶
  ~~~

- åœ¨ç®¡é“æ–‡ä»¶ä¸­æ¥æ”¶

  ~~~python
  class MeizituPipeline:
      def process_item(self, item, spider):
          title = item["title"]
          urls = item["urls"]
  ~~~

### 2. æ–‡ä»¶ä¸‹è½½

æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥åœ¨ç®¡é“æ–‡ä»¶ä¸­å®šä¹‰ä¸‹è½½è¿‡ç¨‹äº†

~~~python
import os
import random
import requests

# é…ç½®éšæœºè¯·æ±‚å¤´è¿›è¡Œä¼ªè£… è¶Šå¤šè¶Šå¥½
request_headers = [
    "Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0",
    ...
]


class MeizituPipeline:
    def process_item(self, item, spider):
        title = item["title"]
        urls = item["urls"]
        for url in urls:
            # å¤„ç†æ¯ä¸€ä¸ªå›¾ç‰‡çš„èµ„æºé“¾æ¥
            os.chdir("D:/pic")
            # æ”¹å˜å·¥ä½œè·¯å¾„
            file_name = url.split(".com/")[1].replace("/", "-")
            # æå–æ–‡ä»¶åç§°
            image = requests.get(url,
                                 headers={'User-Agent': random.choice(request_headers),
                                          'referer': 'https://www.mzitu.com/'})
            # åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ  referer ä¼ªè£…çˆ¬è™«
            f = open(file_name, 'wb')
            f.write(image.content)
            f.close()
~~~

> ç„¶åå°±å¯ä»¥å¼€å§‹äº«å—æˆæœäº†ï¼

